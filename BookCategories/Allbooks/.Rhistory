library(tm)
library(stringi)
?tm
?stringi
setwd("~/Desktop/ProjectRepo/group-project-242/Books")
files <- list.files()
test <- "yeah what do you want"
test
test1 <- c("yeah", "what", "do", "you", "want")
class(test)
typeof(test)
class(test1)
typeof(test1)
?stri_flatten
stri_flatten(test1)
?stri_paste
stri_paste(test1)
stri_paste(test)
stri_paste(test1,test)
test <- readlines(files[1])
test <- readLines(files[1])
head(test)
articles <- character(length(files))
head(articles)
books<- character(length(files))
?stri_flatten
files <- list.files()
books<- character(length(files))
for ( i in 1:length(books)){
books[i] <- stri_flatten(readLines(files[1]))
}
head(books[[1]])
head(books[[2]])
cat("\014")
head(books[[2]])
cat("\014")
head(books[[3]])
files <- list.files()
files
books<- character(length(files))
length(books)
for ( i in 1:length(books)){
books[i] <- stri_flatten(readLines(files[i]))
}
head(books[[3]])
docs <- Corpus(VectorSource(books))
test <- "shiv shov ?+ nog ..."
test
stri_replace_all_regex(test, "<.+?>", " ")
?stri_replace_all_regex
stri_replace_all_regex(test, ".+?", " ")
stri_replace_all_regex(test, "<.+?>", " ")
test <- "shiv++++ nog????? bob....."
stri_replace_all_regex(test, "<.+?>", " ")
test <- shiv?
test <- "shiv?"
stri_replace_all_regex(test, "<.+?>", " ")
test
stri_replace_all_regex(test, "<.+?>", " ")
?content_transformer
content_transformer(stri_replace_all_regex(test, "<.+?>", " "))
docs2 <- tm_map(docs, content_transformer(function(z) stri_replace_all_regex(z, "<.+?>", " ")))
?content_transformer
??content_transformer
tm_map(content_transformer(stri_replace_all_regex(test, "<.+?>", " ")))
docs2 <- tm_map(docs, content_transformer(function(z) stri_replace_all_regex(z, "<.+?>", " ")))
docs3 <- tm_map(docs2, content_transformer(function(x) stri_replace_all_fixed(x, "\t", " ")))
docs4 <- tm_map(docs3, PlainTextDocument)
docs5 <- tm_map(docs4, stripWhitespace)
docs6 <- tm_map(docs5, removeWords, stopwords("english"))
docs7 <- tm_map(docs6, removePunctuation)
docs8 <- tm_map(docs7, content_transformer(tolower))
head(docs8[[1]])
docs <- Corpus(VectorSource(books))
docs4 <- tm_map(docs, PlainTextDocument)
docs5 <- tm_map(docs4, stripWhitespace)
docs6 <- tm_map(docs5, removeWords, stopwords("english"))
docs7 <- tm_map(docs6, removePunctuation)
docs8 <- tm_map(docs7, content_transformer(tolower))
?removeWords
?stopWords
?stopwrods
?stopwords
head(docs[[7]])
docs <- Corpus(VectorSource(books))
inspect(docs[1:2])
?tm_map
docs4 <- tm_map(docs, PlainTextDocument)
docs5 <- tm_map(docs4, stripWhitespace)
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removePunctuations)
getTransformations()
head(docs[[1]])
inspect(docs[1])
docs <- Corpus(VectorSource(books))
head(docs[[1]])
docs <- tm_map(docs, PlainTextDocument)
head(docs[[1]])
docs <- tm_map(docs, stripWhitespace)
head(docs[[1]])
head(docs[[1]])
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, tolower)
head(docs[[1]])
docs <- tm_map(docs, removeWords, stopwords("english"))
?stopwords
?removePunctuations
?removePunctuation
docs <- tm_map(docs, removePunctuation)
head(docs[[1]])
setwd("~/Desktop/ProjectRepo/group-project-242/Data")
#loading in sentiments
senti <- read.table("NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt", stringsAsFactors = FALSE )
colnames(senti) <- c("word", "emotion", "score")
Uword <- unique(senti$word)
head(Uword)
DTM <- DocumentTermMatrix(reuters, list(dictionary = Uword))
DTM <- DocumentTermMatrix(docs, list(dictionary = Uword))
head(Uword)
Uword[1]
?DocumentTermMatrix
DTM <- DocumentTermMatrix(docs, control = list(dictionary = Uword))
head(inspect(DocumentTermMatrix(docs, control = list(dictionary = Uword))))
TDM <- TermDocumentMatrix(docs, control = list(dictionary = Uword))
?dictionary
TDM <- TermDocumentMatrix(docs)
head(DTM[[1]])
head(DTM[1])
DTM
TDM <- DocumentTermMatrix(docs, list(dictionary=Uword))
warning()
inspect(TDM)
install.packages("NLP")
library(NLP)
?NLP
TDM <- DocumentTermMatrix(docs, list(dictionary=Uword))
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, stripWhitespace)
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- DocumentTermMatrix(docs, list(dictionary=Uword))
install.packages("SnowballC")
library(SnowballC)
TDM <- DocumentTermMatrix(docs, list(dictionary=Uword))
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
t(as.matrix(docsTDM))[,sample(10000,10)]
t(as.matrix(TDM))[,sample(10000,10)]
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, content_transformer(function(z) stri_replace_all_regex(z, "<.+?>", " ")))
head(docs[[1]])
docs <- tm_map(docs, content_transformer(stri_replace_all_regex(z, "<.+?>", " ")))
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
wordmatrix <- t(as.matrix(TDM)
)
head(wordmatrix)
wordmatrix[1:10,1:10]
wordmatrix[,1:10]
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
inspect(TDM)
head(inspect(TDM))
library(stringi)
files <- list.files()
files
setwd("~/Desktop/ProjectRepo/group-project-242/Books")
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- str_flatten(readLines(stri_paste(files[i])), col = " ")
}
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
head(books[[1]][1:5])
head(inspect(TDM))
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
wordmatrix <- t(as.matrix(TDM))
?dist
?dist
dissim <- dist(wordmatrix, method = 'cose')
dissim <- dist(wordmatrix, method = 'cosine')
dissim <- dist(wordmatrix, method = "cosine")
library(proxy)
install.packages("proxy")
library(proxy)
dissim <- dist(wordmatrix, method = "cosine")
head(dissim[,1:10])
dissim
h <- hclust(dissim, method = "average")
plot(h, labels = titles)
plot(h, labels = files)
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories")
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
wordmatrix <- t(as.matrix(TDM))
rm(docs2)
rem(docs3)
rm(docs3)
rm(docs4)
rm(docs5)
rm(docs6)
rm(docs7)
rm(docs8)
rm(docs9)
rm(test)
rm(test1)
rm(DTM)
rm(articles)
TDM[,1:10]
wordmatrix[,1:10]
dissim <- dist(wordmatrix, method = "cosine")
h <- hclust(dissim, method = "average")
plot(h, labels = files)
hc.average<- hclust(dissim, method = "average")
hc.complete=hclust(dissim, method="complete")
hc.single=hclust(dissim, method="single")
par(mfrow(1:3))
plot(hc.average, labels = files)
par(mfrow=c(1,3))
plot(hc.average, labels = files)
plot(hc.complete, labels = files)
plot(hc.single, labels = files)
wordmatrix[,0:5]
library(class)
?Direction
?direction
train <- wordmatrix
list.files(0)
list.files()
list.files()
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Training")
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Training")
list.files()
head(wordmatrix[,1:10])
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
list.files()
head(docs[[1]])
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
training <- t(as.matrix(TDM))
rm(training)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
train <- t(as.matrix(TDM))
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Test")
files <- list.files()
files
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
train <- t(as.matrix(TDM))
test <- t(as.matrix(TDM))
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Training")
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
train <- t(as.matrix(TDM))
list.files()
trainnames <- c("music", "philosophy", "religion", "music", "religion", "religion", "crime", "philosophy", "philosophy", "philosophy", "religion", "religion", "music", "music", "crime", "music", "philosophy", "crime", "crime", "crime")
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Test")
list.files()
testnames <- c("crime", "religion", "philosophy", "music", "music", "crime", "crime", "philosophy", "music", "religion", "crime", "philosophy", "religion", "religion", "crime", "philosophy", "music", "music", "philosophy", "religion")
head(cbind(trainnames, train))
test <-cbind(trainnames, train)
test[,1:10]
train[,1:10]
test <- as.integer(test)
head(test[,1:10])
test[,1:10]
test <- data.frame(trainnames, train)
test[,1:10]
list.files()
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
testing <- t(as.matrix(TDM))
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Training")
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
training <- t(as.matrix(TDM))
testing_names <- data.frame(trainnames, training)
training_names <- data.frame(testnames, testing)
testing_names[,1:10]
training_names <- data.frame(trainnames, training)
testing_names <- data.frame(testnames, testing)
testing_names[,1:10]
sample(1:5,5)
k <- 5
set.seed(1234)
shuffle <- sample( 1 : nrow(training_names), nrow(training_names))
split.size <- round(length(shuffle) / k)
groups <- rep(1 : k, each = split.size)
head(groups)
groups
k <- 5
set.seed(1234)
shuffle <- sample( 1 : nrow(training_names), nrow(training_names))
split.size <- round(length(shuffle) / k)
split.size
nrow(training_names)
shuffle <- sample( 1 : nrow(training_names), nrow(training_names))
shuffle
split.size <- round(length(shuffle) / k)
split.size
groups <- rep(1 : k, each = split.size)
split.ind <- split(shuffle, groups)
split.ind
miss.rate.i <- rep( NA, k)
miss.rate.i
k.knn <- 15
miss.rate.cv <- rep( NA, k.knn)
test <- knn(training_names[,-1], testing_names[,-1], train[,1], k = 5)
test
trainnames <- as.factor(c("music", "philosophy", "religion", "music", "religion", "religion", "crime", "philosophy", "philosophy", "philosophy", "religion", "religion", "music", "music", "crime", "music", "philosophy", "crime", "crime", "crime"))
testnames <- as.factor(c("crime", "religion", "philosophy", "music", "music", "crime", "crime", "philosophy", "music", "religion", "crime", "philosophy", "religion", "religion", "crime", "philosophy", "music", "music", "philosophy", "religion"))
levels(trainnames)
levels(testnames)
training_names <- data.frame(trainnames, training)
testing_names <- data.frame(testnames, testing)
test <- knn(training_names[,-1], testing_names[,-1], train[,1], k = 5)
test
test <- knn(training_names[,-1], testing_names[,-1], training_names[,1], k = 5)
test
testing_names
test <- knn(training_names[,-1], testing_names[,-1], training_names[,1], k = 5)
test
testnames
test == testnames
mean(test == testnames)
miss.rate <- rep(NA, k.knn)
k.knn <- 15
miss.rate <- rep(NA, k.knn)
for(i in 1:k.knn){
test <- knn(training_names[,-1], testing_names[,-1], training_names[,1], k = 5)
miss.rate[i] <- mean(test == testnames)
}
miss.rate[i]
miss.rate
miss.rate
k.knn <- 15
miss.rate <- rep(NA, k.knn)
for(i in 1:k.knn){
test <- knn(training_names[,-1], testing_names[,-1], training_names[,1], k = i)
miss.rate[i] <- mean(test == testnames)
}
miss.rate
knn.predict <- knn(training_names[,-1], testing_names[,-1], training_names[,1], k = 1)
knn.predict
table(knn.predict, testnames)
setwd("~/Desktop/ProjectRepo/group-project-242/BookCategories/Allbooks")
files <- list.files()
files <- list.files()
books <- character(length(files))
for(i in 1:length(files)){
books[i] <- stri_flatten(readLines(stri_paste(files[i])), col = " ")
}
docs <- Corpus(VectorSource(books))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, PlainTextDocument)
TDM <- TermDocumentMatrix(docs, list(dictionary=Uword))
wordmatrix <- t(as.matrix(TDM))
dissim <- dist(wordmatrix, method = "cosine")
hc.average<- hclust(dissim, method = "average")
hc.complete=hclust(dissim, method="complete")
hc.single=hclust(dissim, method="single")
plot(hc.average, labels = files)
plot(hc.average, labels = files)
books[[1]]
books[[1]]
docs <- Corpus(VectorSource(books))
docs[[1]]
docs[[1]]
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, tolower)
books[[1]]
books[[1]]
plot(hc.single, labels = files)
hc.complete=hclust(dissim, method="complete")
plot(hc.complete, labels = files)
?class
?proxy
library(proxy)
?proxy
fix(files)
library(randomForest)
2+2
5+5
training_names[,1:10]
randomF.fit = randomForest(as.factor(training_names$trainname)~., data = training_names, mtry = round(sqrt(ncol(training)-1)), importance = TRUE); randomF.fit
save.image("~/Desktop/projwork.RData")
